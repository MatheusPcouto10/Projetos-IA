{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"Taxi-v3.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"normal-liechtenstein"},"source":["import gym\n","import random\n","import numpy as np\n","from time import sleep\n","from IPython.display import clear_output"],"id":"normal-liechtenstein","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ZX4_921u6Uk"},"source":["# Método de Exibir o estado atual"],"id":"3ZX4_921u6Uk"},{"cell_type":"code","metadata":{"id":"extreme-philippines"},"source":["def exibir(*args):\n","    clear_output(wait=True)\n","    if env is not None:\n","        env.render() # Renderiza o estado atual do ambiente\n","\n","    if len(args) > 0:\n","        for arg in args:\n","            print(arg, end=\" \")\n","        print(\"\")\n","    sleep(0.3)"],"id":"extreme-philippines","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zzm1PDDEuqhe"},"source":["## Criação da Tabela de Valores\n","\n","Inicializar a tabela-Q preenchida com zeros.\n","\n","Para as ações seguintes, há uma chance do agente tomá-las de forma aleatória ou seguir a política da tabela Estados x Ações.\n","Após episódios suficientes, é esperado que o agente atinja uma política ótima e a tabela Estados x Ações esteja com os valores o mais próximo do ideal possível."],"id":"zzm1PDDEuqhe"},{"cell_type":"code","metadata":{"id":"surrounded-travel"},"source":["env = gym.make(\"Taxi-v3\")\n","env.reset() # Reseta o ambiente e retorna um estado inicial aleatório\n","tabela_q = np.zeros([env.observation_space.n, env.action_space.n]) #  A tabela recebe o espaço de ações e espaço de estados, é iniciada com [0,0]"],"id":"surrounded-travel","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A9eBW6wZuz32"},"source":["# Treinamento do Algoritmo\n","\n","α: quanto mais próximo do fim do treino, com uma base de conhecimento sobre o ambiente já consolidada, uma alta taxa de aprendizado torna-se menos relevante.\n","\n","γ: Quanto mais próximo do final do treinamento, o comportamento do agente para os finais dos episódios já estão bem definido e, se estiver aprendendo de forma correta, estará de acordo com o esperado. É interessante então melhorar o comportamento para as situações mais iniciais.\n","\n","ε: Quanto mais atualizada a tabela-q, menos o agente precisa explorar ações aleatórias.\n"],"id":"A9eBW6wZuz32"},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"standing-special","executionInfo":{"status":"ok","timestamp":1616465895964,"user_tz":180,"elapsed":122009,"user":{"displayName":"Matheus Pimentel do Couto","photoUrl":"","userId":"16307449855433180712"}},"outputId":"1ab72bda-7443-40de-a802-d65909b84e61"},"source":["# variáveis de controle\n","alpha = 0.1 # Taxa de aprendizado\n","gamma = 0.6 # Taxa de desconto\n","epsilon = 0.1 # Chance de selecionar uma ação aleatória em vez de maximizar o prémio. Exploration\n","episodios_totais, penalidades_totais = 0, 0 \n","\n","episodios = 20000\n","\n","for i in range(episodios):\n","    feito = False # verifica se o treinamento terminou\n","    penalidade, premiacao = 0, 0\n","    estado = env.reset()\n","\n","    while not feito:\n","        if random.uniform(0,1) < epsilon: # decidindo se será tomado uma ação aleatória ou se seguirá a política da tabela-q\n","            acao = env.action_space.sample()\n","        else:\n","            acao = np.argmax(tabela_q[estado])\n","\n","        proximo_estado, premiacao, feito, info = env.step(acao) # As variáveis recebem a ação executada\n","\n","        valor = tabela_q[estado, acao]\n","        maior_valor = np.max(tabela_q[proximo_estado])\n","\n","        novo_valor = (1 - alpha) * valor + alpha * (premiacao + gamma * maior_valor) # atualizando o valor de q a partir da equação de Bellman\n","        tabela_q[estado, acao] = novo_valor # colocando este valor na tabela-q\n","\n","        if premiacao == -10: # contabilizando os embarques/desembarques errados\n","            penalidade += 1\n","\n","        episodios += 1\n","        estado = proximo_estado\n","\n","        if i % 100 == 0:\n","            print(f\"Episódio: {i}\")\n","            clear_output(wait=True)\n","        else:\n","            exibir(acao, proximo_estado, premiacao, penalidades_totais, episodios, info)\n","\n","    penalidades_totais += penalidade\n","    episodios_totais += episodios\n","\n","print(\"Treinamento finalizado.\")\n","\n","print(f\"Resutados depois de {episodios} episodios:\")\n","print(f\"Média de passos por episódio: {episodios_totais / episodios}\")\n","print(f\"Média de penalidades por episódio: {penalidades_totais / episodios}\")"],"id":"standing-special","execution_count":null,"outputs":[{"output_type":"stream","text":["+---------+\n","|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n","| : | : : |\n","| : : : : |\n","| | : | : |\n","|\u001b[43mY\u001b[0m| : |B: |\n","+---------+\n","  (East)\n","2 401 -1 13 603 {'prob': 1.0, 'TimeLimit.truncated': True} \n","Treinamento finalizado.\n","Resutados depois de 603 episodios:\n","Média de passos por episódio: 2.0049751243781095\n","Média de penalidades por episódio: 0.03814262023217247\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"g8rHzKYq4Ltu"},"source":["# Salva as informações da Tabela_q"],"id":"g8rHzKYq4Ltu"},{"cell_type":"code","metadata":{"id":"experienced-supply"},"source":["np.save(\"arquivo_tabela_q\", tabela_q)\n","tabela_q = np.load(\"arquivo_tabela_q.npy\")"],"id":"experienced-supply","execution_count":null,"outputs":[]}]}